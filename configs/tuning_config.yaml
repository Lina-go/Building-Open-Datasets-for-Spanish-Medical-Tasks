# Configuración para tuning de hiperparámetros con wandb sweeps
# Ejecuta:
#   python -m src.scripts.tune --config configs/tuning_config.yaml --runs 20

model_name: dccuchile/bert-base-spanish-wwm-cased

data:
  base_dir: data
  cleaning_type: splits_extra_cleaned
  files:
    train: train_multilabel.csv
    val: val_multilabel.csv
    test: test_multilabel.csv

text:
  mode: title_abstract                   # (title_abstract, abstract_only, title_only)
  title_column: title_processed
  abstract_column: spanish_abstract_processed
  title_fallback: title
  abstract_fallback: spanish_abstract

train:
  max_length: 512
  early_stopping_patience: 3
  early_stopping_delta: 0.001

wandb:
  project: medical-text-classification-tuning
  entity: null
  mode: online                           # (online, offline, disabled)

# Definición del sweep (Bayesian por defecto)
sweep:
  method: bayes
  metric: {name: "eval/f1_weighted", goal: maximize}
  parameters:
    learning_rate: {distribution: log_uniform_values, min: 1.0e-6, max: 1.0e-3}
    batch_size: {values: [8, 16, 32, 48]}
    num_epochs: {values: [3, 4, 5, 6]}
    warmup_ratio: {distribution: uniform, min: 0.0, max: 0.2}
    weight_decay: {distribution: uniform, min: 0.0, max: 0.3}
    gradient_accumulation_steps: {values: [1, 2, 4]}
    lr_scheduler_type: {values: [linear, cosine, cosine_with_restarts, polynomial]}
    max_grad_norm: {distribution: uniform, min: 0.5, max: 2.0}
    dropout_rate: {distribution: uniform, min: 0.1, max: 0.5}
    attention_dropout: {distribution: uniform, min: 0.1, max: 0.3}